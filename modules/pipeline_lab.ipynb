{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be76938e-1bc9-40b7-be16-8f840c6b6830",
   "metadata": {},
   "source": [
    "## Notebook para el doblaje manual del audio dado un vídeo de YouTube\n",
    "##### NOTA: borrar directorio correspondiente al audio antes de iniciar\n",
    "##### NOTA: debe tener el waves.styletts corriendo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d85c96-01f5-4de2-901e-5970c2a80403",
   "metadata": {},
   "source": [
    "### Importa librerías de Python y Modelo Whisper Large V3 \n",
    "#### En macOS usa 'mps' como gpu y cuda en caso de disponer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396639d3-a30d-4854-9e47-24639a7816a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
      "Using cache found in /Users/beltre.wilton/.cache/torch/hub/snakers4_silero-vad_master\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import re\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import requests\n",
    "import scipy.signal as signal\n",
    "import stable_whisper\n",
    "import torch\n",
    "import whisperx\n",
    "from fastapi import APIRouter, Depends, HTTPException, status\n",
    "from pydub import AudioSegment\n",
    "from pytube import YouTube\n",
    "from sqlalchemy.orm import Session\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    ")\n",
    "\n",
    "from IPython.display import display, Audio, Video\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from dbms.database import get_db\n",
    "from modules.combine import combine_chunks\n",
    "from modules.demucs_impl import demucs_it\n",
    "from modules.silero_based_chunk import chunk, split_audio\n",
    "\n",
    "\n",
    "parent = Path(os.getcwd()).parent.__str__()\n",
    "\n",
    "output_path = f\"{parent}/audios\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\"\n",
    "model_name_hf = \"openai/whisper-large-v3\"\n",
    "\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_name_hf, device=device,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb676d-31ca-43f0-b416-2b105614dd47",
   "metadata": {},
   "source": [
    "## Grupo de funciones encargadas de hacer una operación puntual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df00b1b-3fe3-4f9d-86c6-2ba7d86bd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp4_to_wav(input_mp4, output_wav):\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_mp4):\n",
    "        print(f\"Error: Input file '{input_mp4}' not found.\")\n",
    "        return\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', input_mp4,\n",
    "        # '-ss', '00:01:28',\n",
    "        # '-to', '00:05:57',\n",
    "        '-acodec', 'pcm_s16le',\n",
    "        '-hide_banner',\n",
    "        '-loglevel', 'error',\n",
    "        output_wav\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Error ffmpeg encontrado durante ejecucion, {result.stderr}\")\n",
    "        print(f\"Conversion successful: {input_mp4} -> {output_wav},\\n{result.stderr} {result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during conversion: {e}\")\n",
    "\n",
    "\n",
    "def stereo_to_mono(data):\n",
    "    \"\"\"Convert stereo audio to mono by averaging the samples\"\"\"\n",
    "    mono_audio = np.mean(data, axis=1, dtype=data.dtype)\n",
    "    return mono_audio\n",
    "\n",
    "\n",
    "def resample(data, orig_sr, target_sr):\n",
    "    \"\"\"resample down/up a data audio\"\"\"\n",
    "    ratio = orig_sr / target_sr\n",
    "    nums = int(len(data) / ratio)\n",
    "    if len(data.shape) > 1:\n",
    "        data = stereo_to_mono(data)\n",
    "    # data = zscore(data)\n",
    "    sampled = signal.resample(data, nums)\n",
    "    return sampled\n",
    "\n",
    "def tr_audio_pipe(sr, audio_reps, resample_=False):\n",
    "    if resample_:\n",
    "        audio_reps = resample(audio_reps, sr, 16_000)\n",
    "\n",
    "    r = pipe(audio_reps, return_timestamps=True, chunk_length_s=30, stride_length_s=[4, 2], batch_size=8,\n",
    "             generate_kwargs = {\"language\":\"<|es|>\",\"task\": \"translate\"})\n",
    "    return r['text']\n",
    "\n",
    "\n",
    "def dl_audio(url: str):\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    ytb_key, pathfile = \"\", \"\"\n",
    "    if \"?\" in url:\n",
    "        ytb_key = url.split(\"?\")[1]\n",
    "        ytb_key = ytb_key.replace(\"v=\", \"\") if \"v=\" in ytb_key else ytb_key\n",
    "        os.makedirs(f\"{output_path}/{ytb_key}\", exist_ok=True)\n",
    "        pathfile = f\"{output_path}/{ytb_key}/{ytb_key[2:]}.mp4\"\n",
    "        video_file = f\"{output_path}/{ytb_key}/{ytb_key[2:]}.mp4\"\n",
    "    else:\n",
    "        ytb_key = url.split(\"shorts/\")[1]\n",
    "        os.makedirs(f\"{output_path}/{ytb_key}\", exist_ok=True)\n",
    "        pathfile = f\"{output_path}/{ytb_key}/{ytb_key}.mp4\"\n",
    "        video_file = f\"{output_path}/{ytb_key}/{ytb_key[2:]}.mp4\"\n",
    "\n",
    "    if not os.path.exists(pathfile):\n",
    "        yt = YouTube(url)\n",
    "        video = yt.streams.filter(only_audio=True).first()\n",
    "        out_file = video.download(output_path=output_path)\n",
    "        os.rename(out_file, pathfile)\n",
    "\n",
    "        # VIDEO\n",
    "        video = yt.streams.filter(only_audio=False).first()\n",
    "        video_file = video.download(output_path=output_path)\n",
    "        os.rename(video_file, pathfile.replace(\".mp4\", \"_V.mp4\"))\n",
    "\n",
    "\n",
    "    audio = AudioSegment.from_file(pathfile, format=\"mp4\")\n",
    "    # audio_base64 = base64.b64encode(audio.export(format=\"wav\").read())\n",
    "    # return audio_base64.decode(\"utf-8\")\n",
    "    # static_synth_url = f\"https://127.0.0.1:8000/{pathfile.replace('.mp4', '_PART__synth.wav')}\"\n",
    "    static_url = f\"https://127.0.0.1:8000/{pathfile}\"\n",
    "    return static_url, pathfile, audio.duration_seconds, audio.frame_rate, audio.channels\n",
    "\n",
    "\n",
    "def get_audio_info(pathfile):\n",
    "    audio = AudioSegment.from_file(pathfile, format=\"wav\")\n",
    "    return pathfile, audio.duration_seconds, audio.frame_rate, audio.channels\n",
    "\n",
    "\n",
    "def synth_req(audio_path: str, text: str, alpha: float = 0.3, beta: float = 0.2, use_vc = True) -> dict:\n",
    "    data = {\n",
    "        \"audio_path\": audio_path,\n",
    "        \"text\": text,\n",
    "        \"alpha\":  alpha,\n",
    "        \"beta\":  beta,\n",
    "        \"use_vc\": use_vc,\n",
    "    }\n",
    "    url = \"https://127.0.0.1:8060/tts/synth\"\n",
    "    public_pem = os.getcwd() + '/certs/public.crt'\n",
    "    key_pem = os.getcwd() + '/certs/key.pem'\n",
    "    r = requests.post(url=url, data=json.dumps(data), verify=False)\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "def tr_chunks(audio_path: Path, cuts: dict) -> dict:\n",
    "    key = audio_path.parent.name\n",
    "    vocals = audio_path.parent\n",
    "    trs = cuts.copy()\n",
    "    for i, v in enumerate(sorted(vocals.rglob(\"vocals_*.wav\"))):\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(f\"translate {v}\")\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        data, sr = librosa.load(v, sr=16_000, mono=True)  # HACE NORMALIZACION\n",
    "        # transcript = tr_audio(sr, data, resample_=False)\n",
    "        start_time = time.time()\n",
    "        transcript = tr_audio_pipe(sr, data)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        trs[i]['path'] = v.absolute().__str__()\n",
    "        trs[i]['transcript'] = transcript\n",
    "    return trs\n",
    "\n",
    "\n",
    "def synth_chunks(trs: dict, use_vc = True):\n",
    "    for i in trs.items():\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        print(f'synthetize {i[1][\"path\"]}')\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        r = synth_req(audio_path=i[1]['path'], text=i[1]['transcript'], use_vc = use_vc)\n",
    "        print(r)\n",
    "\n",
    "\n",
    "#TODO: pendiente impl este dolor!!\n",
    "# mask [{'start', 1.87, 'end': 4.67}, {'start', 15.00, 'end': 18.97}]\n",
    "def mask_seq(cuts: dict, mask: dict):\n",
    "    cutlist = [item[1] for item in cuts.items()]\n",
    "    cutlist.insert(0, {})\n",
    "    for j, item in enumerate(cutlist):\n",
    "        # if item['start']\n",
    "        pass\n",
    "    cuts = {j: item for j, item in enumerate(cutlist)}\n",
    "    return cuts\n",
    "\n",
    "\n",
    "def len_ctrl(trs: dict) -> dict:\n",
    "    #TODO pasaste de los x segundos?, pues:\n",
    "    # buscar el punto mas cerca de la mitad y dividir el texto\n",
    "    # si no hay punto, la coma\n",
    "    # contar la cantidad de palabras en ambas mitades:\n",
    "    # cortar el audio de acuerdo a la proporcion del conteo de palabras en el tiempo total del audio.\n",
    "    # ya que el trs fue translated en su unidad, no pierde contexto, HAY QUE MOVER EL silence TAMBIEN\n",
    "    # TODO usar lista de dict para usar insert con posiciones fijas.\n",
    "    trs_copy = trs.copy()\n",
    "    threshold = 29.0\n",
    "    for i in range(len(trs_copy)):\n",
    "        if trs_copy[i]['length'] >= threshold:\n",
    "            transcript = trs_copy[i]['transcript'].strip()\n",
    "            s = re.split('\\s', transcript.strip())\n",
    "            n = math.floor(len(s) / 2)\n",
    "            text_1 = \" \".join(s[:n])\n",
    "            text_2 = \" \".join(s[n:])\n",
    "            br = trs_copy[i]['length'] * .50 # also part_1_length\n",
    "            part_1_start = 0 # trs_copy[i]['start']\n",
    "            part_1_end = part_1_start + br\n",
    "            part_1_silence = trs_copy[i]['silence']\n",
    "\n",
    "            part_2_silence = 0.050\n",
    "            part_2_start = part_1_end + part_2_silence\n",
    "            part_2_end = trs_copy[i]['end']\n",
    "            part_2_path = f\"{Path(trs_copy[i]['path']).parent}/vocals_{i + 1}.wav\"\n",
    "\n",
    "            p1 = {'start': part_1_start, 'end': part_1_end, 'length': br, 'silence': part_1_silence, 'cut': True,\n",
    "                  'path': trs_copy[i]['path'],\n",
    "                  'transcript': f\"{text_1}.\"}\n",
    "\n",
    "            p2 = {'start': part_2_start, 'end': part_2_end, 'length': (part_2_end - part_2_start), 'silence': part_2_silence, 'cut': True,\n",
    "                  'path': part_2_path,\n",
    "                  'transcript': text_2}\n",
    "\n",
    "            parent = Path(trs_copy[i]['path']).parent\n",
    "\n",
    "            trslist = [item[1] for item in trs.items()]\n",
    "            trslist[i] = p1\n",
    "            trslist.insert(i + 1, p2)\n",
    "            for j, item in enumerate(trslist):\n",
    "                item['path'] = f\"{parent}/vocals_{j}.wav\"\n",
    "            trs = {j: item for j, item in enumerate(trslist)}\n",
    "\n",
    "            for file in itertools.chain(\n",
    "                    parent.rglob(\"vocals_*.wav\"),\n",
    "                    parent.rglob(\"vocals-silence*.wav\"),\n",
    "            ):\n",
    "                file.unlink(missing_ok=True)\n",
    "\n",
    "            vocals = f\"{parent}/vocals.wav\"\n",
    "            split_audio(trs, vocals)\n",
    "\n",
    "    return trs\n",
    "\n",
    "\n",
    "def merge_video(input_video: str, final_sound: str, output_video: str):\n",
    "    # ffmpeg -y -i AiXMnjCo_sU_V.mp4 -i final_sound.wav -c:v copy -c:a aac -map 0:v:0 -map 1:a:0 output.mp4\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', input_video,\n",
    "        '-i', final_sound,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-map', '0:v:0',\n",
    "        '-map', '1:a:0',\n",
    "        '-hide_banner',\n",
    "        '-loglevel', 'error',\n",
    "        output_video\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError(f\"Error ffmpeg encontrado durante ejecucion, {result.stderr}\")\n",
    "        print(f\"Merged successful\\n{result.stderr} {result.stdout}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during conversion: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe3bdd-0721-410d-823e-aa7939bf604c",
   "metadata": {},
   "source": [
    "## Pasos Secuenciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3ba30-ab6a-4d75-9cfa-cd3c45994356",
   "metadata": {},
   "source": [
    "### funciones de los pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cc6e81-0839-4836-b227-56083f0a4504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuts_step(url: str) -> (dict, str, Path):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        static_url, mp4_file, duration_seconds, frame_rate, channels = dl_audio(url) # dl & calc el wav nativo\n",
    "        output_wav = mp4_file.replace(\".mp4\", \".wav\")\n",
    "        mp4_to_wav(input_mp4=mp4_file, output_wav=output_wav)\n",
    "        audio_path = Path(output_wav)\n",
    "\n",
    "        demucs_it(audio_path.absolute().__str__())\n",
    "        vocals = f\"{audio_path.parent.absolute().__str__()}/vocals.wav\"\n",
    "\n",
    "        cuts = chunk(vocals)\n",
    "        return cuts, vocals, audio_path\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "def mask_step(cuts: dict, mask: dict):\n",
    "    pass\n",
    "\n",
    "\n",
    "def tr_step(cuts: dict, vocals: str, audio_path: Path) -> dict:\n",
    "    try:\n",
    "        split_audio(cuts, vocals)\n",
    "        trs = tr_chunks(audio_path, cuts)\n",
    "        trs = len_ctrl(trs)\n",
    "        return trs\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\n",
    "def synth_step(trs: dict, audio_path: Path, use_vc = True) -> str:\n",
    "    synth_chunks(trs, use_vc)\n",
    "    combine_chunks(audio_path)\n",
    "    return f\"{audio_path.parent.absolute().__str__()}/final_sound.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04ff77-1343-41d6-af0d-0fdfb00c78ee",
   "metadata": {},
   "source": [
    "### Paso 'cuts' o de corte se realiza lo siguiente:\n",
    "#### - Se implementa silero \n",
    "#### - Se hace demucs (separación de voz | instrumentales)\n",
    "#### - Implementación de algoritmo propio para mejorar alineamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e8bb19d-4b6a-4825-915c-1148f20ad8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2238) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2239) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2240) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful: /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/onNuYtDJH-U.mp4 -> /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/onNuYtDJH-U.wav,\n",
      " \n",
      "torch.Size([4, 2, 1374208])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'start': 0.066,\n",
       "  'end': 9.662,\n",
       "  'length': 9.596,\n",
       "  'silence': 0.066,\n",
       "  'cut': False},\n",
       " 1: {'start': 10.338,\n",
       "  'end': 29.342,\n",
       "  'length': 19.003999999999998,\n",
       "  'silence': 0.6759999999999984},\n",
       " 2: {'start': 30.274,\n",
       "  'end': 31.1611875,\n",
       "  'length': 0.8871874999999996,\n",
       "  'silence': 0.9320000000000022}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descarga el audio y lo segmenta en uteraciones.\n",
    "link = 'https://www.youtube.com/shorts/OtWmhxrq70c'\n",
    "cuts, vocals, audio_path = cuts_step(link)\n",
    "cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffb516-6048-4445-b104-3519aa9b4bcc",
   "metadata": {},
   "source": [
    "### Paso para agregar una máscara, para que no toque alguna(s) parte del audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae050df8-f363-41d2-b5bb-fc7d9c44fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se requiere agregar mascaras, este es el lugar.\n",
    "mask = {}\n",
    "mask_step(cuts, {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ea682-7d5f-4ceb-862c-707ced844c44",
   "metadata": {},
   "source": [
    "### Transcripción, traducción con Whisper, además:\n",
    "#### - Se divide el audio de vocales en partes pequeñas\n",
    "#### - Algoritmo propio para controlar la longitud de uteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab78fd4f-509b-438f-9ca4-61830c98f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "translate /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_0.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "--- 24.071895837783813 seconds ---\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "translate /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_1.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "--- 13.105316638946533 seconds ---\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "translate /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_2.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "--- 5.920491695404053 seconds ---\n"
     ]
    }
   ],
   "source": [
    "trs = tr_step(cuts, vocals, audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fc090-ac3d-46a0-9e78-c5bbbfe79ab2",
   "metadata": {},
   "source": [
    "### En este punto se puede modificar `trs` para cambiar algo en la traducción que amerite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09580665-2bd5-47e9-b4a0-322d65f4d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'start': 0.066,\n",
       "  'end': 9.662,\n",
       "  'length': 9.596,\n",
       "  'silence': 0.066,\n",
       "  'cut': False,\n",
       "  'path': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_0.wav',\n",
       "  'transcript': \" Look at a shark! Oh, he doesn't want to open his mouth! Keep watching! Look at this big one!\"},\n",
       " 1: {'start': 10.338,\n",
       "  'end': 29.342,\n",
       "  'length': 19.003999999999998,\n",
       "  'silence': 0.6759999999999984,\n",
       "  'path': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_1.wav',\n",
       "  'transcript': \" Keep watching Look how many Ahamaya Keep watching Look at these psychopath shrimp How big Keep watching My people, I already taught you a lot of maripos So you already know Give it a like To cook this psychopath Shark So follow me if you don't follow me\"},\n",
       " 2: {'start': 30.274,\n",
       "  'end': 31.1611875,\n",
       "  'length': 0.8871874999999996,\n",
       "  'silence': 0.9320000000000022,\n",
       "  'path': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_2.wav',\n",
       "  'transcript': ' See you next time.'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47e045f-bf39-4ae6-9b1b-48196a1dbf70",
   "metadata": {},
   "source": [
    "### Sintetizado de audio + combinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efdb2c7b-6bde-4231-821e-91787596f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "synthetize /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_0.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "{'synth_wav_file': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_0_synth_a-0.3_b-0.2_df-10_em-1.wav', 'synth_name': 'vocals_0_synth_a-0.3_b-0.2_df-10_em-1.wav', 'response': 'completed'}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "synthetize /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_1.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "{'synth_wav_file': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_1_synth_a-0.3_b-0.2_df-10_em-1.wav', 'synth_name': 'vocals_1_synth_a-0.3_b-0.2_df-10_em-1.wav', 'response': 'completed'}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "synthetize /Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_2.wav\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "{'synth_wav_file': '/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/vocals_2_synth_a-0.3_b-0.2_df-10_em-1.wav', 'synth_name': 'vocals_2_synth_a-0.3_b-0.2_df-10_em-1.wav', 'response': 'completed'}\n",
      "vocals_0_synth_a-0.3_b-0.2_df-10_em-1.wav\n",
      "vocals_1_synth_a-0.3_b-0.2_df-10_em-1.wav\n",
      "vocals_2_synth_a-0.3_b-0.2_df-10_em-1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2258) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(2259) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "synth_wav_file = synth_step(trs, audio_path, use_vc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f20e2fb3-a8dd-4698-a12a-e479649dce11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/beltre.wilton/apps/waves.api/audios/onNuYtDJH-U/final_sound.wav'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_wav_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9139c-1d11-4f32-bda6-62f9851cac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Audio Sintetizado')\n",
    "display(Audio(synth_wav_file))\n",
    "\n",
    "print('Audio Original')\n",
    "display(Audio(vocals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a56e6e0c-5cec-4a8b-a5f5-e564e027f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(2279) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged successful\n",
      " \n"
     ]
    }
   ],
   "source": [
    "input_video = f\"{audio_path.__str__().replace('.wav', '_V.mp4')}\" \n",
    "output_video = f\"{audio_path.parent}/video_merged.mp4\"\n",
    "\n",
    "merge_video(input_video, synth_wav_file, output_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f9a49ef-4a3f-4ff2-a225-cb1424b5ee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/beltre.wilton/apps/waves.api/audios/pgQBYfbCc6w/video_merged.mp4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0959d-11a4-4f4a-8c9f-c29f3f8a1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from  base64 import b64encode\n",
    "\n",
    "# Show video\n",
    "mp4 = open(output_video,'rb').read()\n",
    "output = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "mp4 = open(input_video,'rb').read()\n",
    "input = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<table>\n",
    "   <tr>\n",
    "      <td>\n",
    "         <h3>Sintetizado</h3>\n",
    "         <video width=400 controls>\n",
    "            <source src=\"{output}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "      </td>\n",
    "      <td>\n",
    "         <h3>Original</h3>\n",
    "         <video width=400 controls>\n",
    "            <source src=\"{input}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "      </td>\n",
    "   </tr>\n",
    "</table>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f3fca-d550-4fe8-b05f-52aef01c70c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
